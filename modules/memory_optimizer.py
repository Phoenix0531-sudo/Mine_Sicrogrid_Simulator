# -*- coding: utf-8 -*-
"""
å†…å­˜ä¼˜åŒ–ç­–ç•¥æ¨¡å— - è§£å†³å¤§æ•°æ®é›†å¤„ç†é—®é¢˜
åŒ…å«æ•°æ®åˆ†å—ã€å†…å­˜ç›‘æŽ§ã€åžƒåœ¾å›žæ”¶ã€æ•°æ®åŽ‹ç¼©ç­‰åŠŸèƒ½
"""

import gc
import psutil
import pandas as pd
import numpy as np
import pickle
import gzip
import tempfile
import os
from typing import Any, Iterator, List, Optional, Union, Callable
from dataclasses import dataclass
from contextlib import contextmanager
import streamlit as st
import logging

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
    total_mb: float
    used_mb: float
    available_mb: float
    percent_used: float
    process_mb: float

class MemoryMonitor:
    """å†…å­˜ç›‘æŽ§å™¨"""
    
    def __init__(self, warning_threshold: float = 80.0, critical_threshold: float = 90.0):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.initial_memory = self.get_memory_stats()
    
    def get_memory_stats(self) -> MemoryStats:
        """èŽ·å–å½“å‰å†…å­˜ç»Ÿè®¡"""
        try:
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()
            
            # å½“å‰è¿›ç¨‹å†…å­˜
            process = psutil.Process()
            process_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            return MemoryStats(
                total_mb=memory.total / 1024 / 1024,
                used_mb=memory.used / 1024 / 1024,
                available_mb=memory.available / 1024 / 1024,
                percent_used=memory.percent,
                process_mb=process_memory
            )
        except Exception as e:
            logger.error(f"èŽ·å–å†…å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return MemoryStats(0, 0, 0, 0, 0)
    
    def check_memory_status(self) -> str:
        """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
        stats = self.get_memory_stats()
        
        if stats.percent_used >= self.critical_threshold:
            return "critical"
        elif stats.percent_used >= self.warning_threshold:
            return "warning"
        else:
            return "normal"
    
    def get_memory_usage_delta(self) -> float:
        """èŽ·å–å†…å­˜ä½¿ç”¨å˜åŒ–"""
        current = self.get_memory_stats()
        return current.process_mb - self.initial_memory.process_mb

class DataFrameOptimizer:
    """DataFrameå†…å­˜ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_dtypes(df: pd.DataFrame, aggressive: bool = False) -> pd.DataFrame:
        """
        ä¼˜åŒ–DataFrameæ•°æ®ç±»åž‹
        
        å‚æ•°:
        - df: è¦ä¼˜åŒ–çš„DataFrame
        - aggressive: æ˜¯å¦ä½¿ç”¨æ¿€è¿›ä¼˜åŒ–
        
        è¿”å›ž:
        - pd.DataFrame: ä¼˜åŒ–åŽçš„DataFrame
        """
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        optimized_df = df.copy()
        
        # ä¼˜åŒ–æ•°å€¼åˆ—
        for col in optimized_df.select_dtypes(include=['int64']).columns:
            col_min = optimized_df[col].min()
            col_max = optimized_df[col].max()
            
            if col_min >= -128 and col_max <= 127:
                optimized_df[col] = optimized_df[col].astype('int8')
            elif col_min >= -32768 and col_max <= 32767:
                optimized_df[col] = optimized_df[col].astype('int16')
            elif col_min >= -2147483648 and col_max <= 2147483647:
                optimized_df[col] = optimized_df[col].astype('int32')
        
        for col in optimized_df.select_dtypes(include=['float64']).columns:
            if aggressive:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºint
                if optimized_df[col].fillna(0) % 1 == 0:
                    optimized_df[col] = optimized_df[col].astype('Int32')
                else:
                    optimized_df[col] = optimized_df[col].astype('float32')
            else:
                optimized_df[col] = optimized_df[col].astype('float32')
        
        # ä¼˜åŒ–å¯¹è±¡åˆ—
        for col in optimized_df.select_dtypes(include=['object']).columns:
            if optimized_df[col].dtype == 'object':
                # å¦‚æžœå”¯ä¸€å€¼æ¯”ä¾‹å°äºŽ50%ï¼Œè½¬æ¢ä¸ºcategory
                if optimized_df[col].nunique() / len(optimized_df) < 0.5:
                    optimized_df[col] = optimized_df[col].astype('category')
        
        optimized_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
        reduction = (original_memory - optimized_memory) / original_memory * 100
        
        logger.info(f"DataFrameå†…å­˜ä¼˜åŒ–: {original_memory:.1f}MB -> {optimized_memory:.1f}MB (å‡å°‘{reduction:.1f}%)")
        
        return optimized_df
    
    @staticmethod
    def chunk_dataframe(df: pd.DataFrame, chunk_size: int = 10000) -> Iterator[pd.DataFrame]:
        """
        å°†DataFrameåˆ†å—å¤„ç†
        
        å‚æ•°:
        - df: è¦åˆ†å—çš„DataFrame
        - chunk_size: æ¯å—çš„å¤§å°
        
        è¿”å›ž:
        - Iterator[pd.DataFrame]: DataFrameå—çš„è¿­ä»£å™¨
        """
        for i in range(0, len(df), chunk_size):
            yield df.iloc[i:i + chunk_size]
    
    @staticmethod
    def reduce_memory_usage(df: pd.DataFrame, columns: List[str] = None) -> pd.DataFrame:
        """
        å‡å°‘ç‰¹å®šåˆ—çš„å†…å­˜ä½¿ç”¨
        
        å‚æ•°:
        - df: DataFrame
        - columns: è¦ä¼˜åŒ–çš„åˆ—ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—
        
        è¿”å›ž:
        - pd.DataFrame: ä¼˜åŒ–åŽçš„DataFrame
        """
        if columns is None:
            columns = df.columns.tolist()
        
        for col in columns:
            if col in df.columns:
                col_type = df[col].dtype
                
                if col_type != 'object':
                    c_min = df[col].min()
                    c_max = df[col].max()
                    
                    if str(col_type)[:3] == 'int':
                        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                            df[col] = df[col].astype(np.int8)
                        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                            df[col] = df[col].astype(np.int16)
                        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                            df[col] = df[col].astype(np.int32)
                    
                    elif str(col_type)[:5] == 'float':
                        if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                            df[col] = df[col].astype(np.float32)
        
        return df

class ChunkedProcessor:
    """åˆ†å—å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 10000, memory_limit_mb: float = 500):
        self.chunk_size = chunk_size
        self.memory_limit_mb = memory_limit_mb
        self.monitor = MemoryMonitor()
    
    def process_large_dataset(self, 
                            data: Union[pd.DataFrame, List], 
                            process_func: Callable,
                            combine_func: Optional[Callable] = None) -> Any:
        """
        å¤„ç†å¤§æ•°æ®é›†
        
        å‚æ•°:
        - data: è¦å¤„ç†çš„æ•°æ®
        - process_func: å¤„ç†å‡½æ•°
        - combine_func: ç»“æžœåˆå¹¶å‡½æ•°
        
        è¿”å›ž:
        - Any: å¤„ç†ç»“æžœ
        """
        results = []
        
        if isinstance(data, pd.DataFrame):
            chunks = DataFrameOptimizer.chunk_dataframe(data, self.chunk_size)
        else:
            chunks = [data[i:i + self.chunk_size] for i in range(0, len(data), self.chunk_size)]
        
        for i, chunk in enumerate(chunks):
            # æ£€æŸ¥å†…å­˜çŠ¶æ€
            memory_status = self.monitor.check_memory_status()
            
            if memory_status == "critical":
                logger.warning("å†…å­˜ä½¿ç”¨çŽ‡è¿‡é«˜ï¼Œè§¦å‘åžƒåœ¾å›žæ”¶")
                gc.collect()
                
                # å†æ¬¡æ£€æŸ¥
                memory_status = self.monitor.check_memory_status()
                if memory_status == "critical":
                    raise MemoryError("å†…å­˜ä¸è¶³ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            
            # å¤„ç†å½“å‰å—
            try:
                result = process_func(chunk)
                results.append(result)
                
                logger.info(f"å·²å¤„ç†å— {i + 1}, å†…å­˜ä½¿ç”¨: {self.monitor.get_memory_stats().process_mb:.1f}MB")
                
            except Exception as e:
                logger.error(f"å¤„ç†å— {i + 1} å¤±è´¥: {e}")
                raise
        
        # åˆå¹¶ç»“æžœ
        if combine_func:
            return combine_func(results)
        elif results and isinstance(results[0], pd.DataFrame):
            return pd.concat(results, ignore_index=True)
        else:
            return results

class MemoryCache:
    """å†…å­˜ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size_mb: float = 100):
        self.max_size_mb = max_size_mb
        self.cache = {}
        self.access_times = {}
        self.sizes = {}
    
    def get(self, key: str) -> Optional[Any]:
        """èŽ·å–ç¼“å­˜é¡¹"""
        if key in self.cache:
            self.access_times[key] = pd.Timestamp.now()
            return self.cache[key]
        return None
    
    def put(self, key: str, value: Any):
        """æ·»åŠ ç¼“å­˜é¡¹"""
        # ä¼°ç®—å¤§å°
        size_mb = self._estimate_size(value)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
        while self._get_total_size() + size_mb > self.max_size_mb and self.cache:
            self._evict_lru()
        
        self.cache[key] = value
        self.access_times[key] = pd.Timestamp.now()
        self.sizes[key] = size_mb
    
    def _estimate_size(self, obj: Any) -> float:
        """ä¼°ç®—å¯¹è±¡å¤§å°ï¼ˆMBï¼‰"""
        try:
            if isinstance(obj, pd.DataFrame):
                return obj.memory_usage(deep=True).sum() / 1024 / 1024
            else:
                return len(pickle.dumps(obj)) / 1024 / 1024
        except:
            return 1.0  # é»˜è®¤1MB
    
    def _get_total_size(self) -> float:
        """èŽ·å–æ€»ç¼“å­˜å¤§å°"""
        return sum(self.sizes.values())
    
    def _evict_lru(self):
        """ç§»é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹"""
        if not self.access_times:
            return
        
        lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        
        del self.cache[lru_key]
        del self.access_times[lru_key]
        del self.sizes[lru_key]
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_times.clear()
        self.sizes.clear()

@contextmanager
def memory_limit(limit_mb: float):
    """å†…å­˜é™åˆ¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    monitor = MemoryMonitor()
    initial_memory = monitor.get_memory_stats().process_mb
    
    try:
        yield monitor
    finally:
        current_memory = monitor.get_memory_stats().process_mb
        memory_used = current_memory - initial_memory
        
        if memory_used > limit_mb:
            logger.warning(f"å†…å­˜ä½¿ç”¨è¶…å‡ºé™åˆ¶: {memory_used:.1f}MB > {limit_mb}MB")
            gc.collect()

def optimize_for_memory(func: Callable):
    """å†…å­˜ä¼˜åŒ–è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        # æ‰§è¡Œå‰æ¸…ç†
        gc.collect()
        
        monitor = MemoryMonitor()
        initial_stats = monitor.get_memory_stats()
        
        try:
            result = func(*args, **kwargs)
            
            # å¦‚æžœç»“æžœæ˜¯DataFrameï¼Œå°è¯•ä¼˜åŒ–
            if isinstance(result, pd.DataFrame):
                result = DataFrameOptimizer.optimize_dtypes(result)
            
            return result
            
        finally:
            # æ‰§è¡ŒåŽæ¸…ç†
            final_stats = monitor.get_memory_stats()
            memory_delta = final_stats.process_mb - initial_stats.process_mb
            
            if memory_delta > 50:  # å¦‚æžœå†…å­˜å¢žé•¿è¶…è¿‡50MB
                logger.info(f"å‡½æ•° {func.__name__} å†…å­˜å¢žé•¿: {memory_delta:.1f}MBï¼Œæ‰§è¡Œåžƒåœ¾å›žæ”¶")
                gc.collect()
    
    return wrapper

def create_memory_monitor_ui():
    """åˆ›å»ºå†…å­˜ç›‘æŽ§UI"""
    monitor = MemoryMonitor()
    stats = monitor.get_memory_stats()
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ðŸ’¾ å†…å­˜ç›‘æŽ§")
    
    # å†…å­˜ä½¿ç”¨çŽ‡
    memory_color = "normal"
    if stats.percent_used > 90:
        memory_color = "red"
    elif stats.percent_used > 80:
        memory_color = "orange"
    else:
        memory_color = "green"
    
    st.sidebar.metric(
        label="ç³»ç»Ÿå†…å­˜ä½¿ç”¨çŽ‡",
        value=f"{stats.percent_used:.1f}%",
        delta=f"{stats.used_mb:.0f}MB / {stats.total_mb:.0f}MB"
    )
    
    st.sidebar.metric(
        label="è¿›ç¨‹å†…å­˜ä½¿ç”¨",
        value=f"{stats.process_mb:.1f}MB"
    )
    
    # å†…å­˜ä¼˜åŒ–æŒ‰é’®
    col1, col2 = st.sidebar.columns(2)
    
    with col1:
        if st.button("ðŸ—‘ï¸ åžƒåœ¾å›žæ”¶", help="å¼ºåˆ¶æ‰§è¡Œåžƒåœ¾å›žæ”¶"):
            gc.collect()
            st.sidebar.success("åžƒåœ¾å›žæ”¶å®Œæˆ")
    
    with col2:
        if st.button("ðŸ“Š å†…å­˜è¯¦æƒ…", help="æ˜¾ç¤ºè¯¦ç»†å†…å­˜ä¿¡æ¯"):
            show_memory_details(stats)

def show_memory_details(stats: MemoryStats):
    """æ˜¾ç¤ºå†…å­˜è¯¦æƒ…"""
    with st.sidebar.expander("ðŸ“Š å†…å­˜è¯¦æƒ…", expanded=True):
        st.write(f"**ç³»ç»Ÿæ€»å†…å­˜**: {stats.total_mb:.0f} MB")
        st.write(f"**å·²ä½¿ç”¨å†…å­˜**: {stats.used_mb:.0f} MB")
        st.write(f"**å¯ç”¨å†…å­˜**: {stats.available_mb:.0f} MB")
        st.write(f"**è¿›ç¨‹å†…å­˜**: {stats.process_mb:.1f} MB")
        
        # å†…å­˜ä½¿ç”¨çŽ‡è¿›åº¦æ¡
        st.progress(stats.percent_used / 100)
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if stats.percent_used > 80:
            st.warning("ðŸ’¡ å»ºè®®ï¼šå†…å­˜ä½¿ç”¨çŽ‡è¾ƒé«˜ï¼Œè€ƒè™‘å‡å°‘æ•°æ®é›†å¤§å°æˆ–é‡å¯åº”ç”¨")
        elif stats.percent_used > 60:
            st.info("ðŸ’¡ å»ºè®®ï¼šå®šæœŸæ‰§è¡Œåžƒåœ¾å›žæ”¶ä»¥é‡Šæ”¾å†…å­˜")
        else:
            st.success("ðŸ’¡ å†…å­˜ä½¿ç”¨æ­£å¸¸")
